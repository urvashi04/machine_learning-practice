{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x    y    z    p               q\n",
       "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
       "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
       "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
       "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
       "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
       "5    5.4  3.9  1.7  0.4     Iris-setosa\n",
       "6    4.6  3.4  1.4  0.3     Iris-setosa\n",
       "7    5.0  3.4  1.5  0.2     Iris-setosa\n",
       "8    4.4  2.9  1.4  0.2     Iris-setosa\n",
       "9    4.9  3.1  1.5  0.1     Iris-setosa\n",
       "10   5.4  3.7  1.5  0.2     Iris-setosa\n",
       "11   4.8  3.4  1.6  0.2     Iris-setosa\n",
       "12   4.8  3.0  1.4  0.1     Iris-setosa\n",
       "13   4.3  3.0  1.1  0.1     Iris-setosa\n",
       "14   5.8  4.0  1.2  0.2     Iris-setosa\n",
       "15   5.7  4.4  1.5  0.4     Iris-setosa\n",
       "16   5.4  3.9  1.3  0.4     Iris-setosa\n",
       "17   5.1  3.5  1.4  0.3     Iris-setosa\n",
       "18   5.7  3.8  1.7  0.3     Iris-setosa\n",
       "19   5.1  3.8  1.5  0.3     Iris-setosa\n",
       "20   5.4  3.4  1.7  0.2     Iris-setosa\n",
       "21   5.1  3.7  1.5  0.4     Iris-setosa\n",
       "22   4.6  3.6  1.0  0.2     Iris-setosa\n",
       "23   5.1  3.3  1.7  0.5     Iris-setosa\n",
       "24   4.8  3.4  1.9  0.2     Iris-setosa\n",
       "25   5.0  3.0  1.6  0.2     Iris-setosa\n",
       "26   5.0  3.4  1.6  0.4     Iris-setosa\n",
       "27   5.2  3.5  1.5  0.2     Iris-setosa\n",
       "28   5.2  3.4  1.4  0.2     Iris-setosa\n",
       "29   4.7  3.2  1.6  0.2     Iris-setosa\n",
       "..   ...  ...  ...  ...             ...\n",
       "120  6.9  3.2  5.7  2.3  Iris-virginica\n",
       "121  5.6  2.8  4.9  2.0  Iris-virginica\n",
       "122  7.7  2.8  6.7  2.0  Iris-virginica\n",
       "123  6.3  2.7  4.9  1.8  Iris-virginica\n",
       "124  6.7  3.3  5.7  2.1  Iris-virginica\n",
       "125  7.2  3.2  6.0  1.8  Iris-virginica\n",
       "126  6.2  2.8  4.8  1.8  Iris-virginica\n",
       "127  6.1  3.0  4.9  1.8  Iris-virginica\n",
       "128  6.4  2.8  5.6  2.1  Iris-virginica\n",
       "129  7.2  3.0  5.8  1.6  Iris-virginica\n",
       "130  7.4  2.8  6.1  1.9  Iris-virginica\n",
       "131  7.9  3.8  6.4  2.0  Iris-virginica\n",
       "132  6.4  2.8  5.6  2.2  Iris-virginica\n",
       "133  6.3  2.8  5.1  1.5  Iris-virginica\n",
       "134  6.1  2.6  5.6  1.4  Iris-virginica\n",
       "135  7.7  3.0  6.1  2.3  Iris-virginica\n",
       "136  6.3  3.4  5.6  2.4  Iris-virginica\n",
       "137  6.4  3.1  5.5  1.8  Iris-virginica\n",
       "138  6.0  3.0  4.8  1.8  Iris-virginica\n",
       "139  6.9  3.1  5.4  2.1  Iris-virginica\n",
       "140  6.7  3.1  5.6  2.4  Iris-virginica\n",
       "141  6.9  3.1  5.1  2.3  Iris-virginica\n",
       "142  5.8  2.7  5.1  1.9  Iris-virginica\n",
       "143  6.8  3.2  5.9  2.3  Iris-virginica\n",
       "144  6.7  3.3  5.7  2.5  Iris-virginica\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.get_dummies(df['q'])\n",
    "df = pd.concat([df,a],axis=1,sort=True)\n",
    "X = df.drop(['Iris-setosa', 'Iris-versicolor','Iris-virginica','q'], axis = 1)\n",
    "y = df[['Iris-setosa', 'Iris-versicolor','Iris-virginica']].values\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Y_test,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x    y    z    p            q  Iris-setosa  Iris-versicolor  \\\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa            1                0   \n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa            1                0   \n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa            1                0   \n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa            1                0   \n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa            1                0   \n",
       "\n",
       "   Iris-virginica  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x    y    z    p               q  Iris-setosa  Iris-versicolor  \\\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica            0                0   \n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica            0                0   \n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica            0                0   \n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica            0                0   \n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica            0                0   \n",
       "\n",
       "     Iris-virginica  \n",
       "145               1  \n",
       "146               1  \n",
       "147               1  \n",
       "148               1  \n",
       "149               1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(model,a0):\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.6862184187966438\n",
      "Accuracy after iteration 0 : 23.333333333333332 %\n",
      "Loss after iteration 100 : 0.4261489719965472\n",
      "Accuracy after iteration 100 : 82.5 %\n",
      "Loss after iteration 200 : 0.3478797343270569\n",
      "Accuracy after iteration 200 : 84.16666666666667 %\n",
      "Loss after iteration 300 : 0.31579051187056184\n",
      "Accuracy after iteration 300 : 85.0 %\n",
      "Loss after iteration 400 : 0.2880089077872636\n",
      "Accuracy after iteration 400 : 87.5 %\n",
      "Loss after iteration 500 : 0.2612111162577757\n",
      "Accuracy after iteration 500 : 88.33333333333333 %\n",
      "Loss after iteration 600 : 0.23648071380179836\n",
      "Accuracy after iteration 600 : 88.33333333333333 %\n",
      "Loss after iteration 700 : 0.21649218712738313\n",
      "Accuracy after iteration 700 : 88.33333333333333 %\n",
      "Loss after iteration 800 : 0.19725358764279682\n",
      "Accuracy after iteration 800 : 90.83333333333333 %\n",
      "Loss after iteration 900 : 0.17958246618406054\n",
      "Accuracy after iteration 900 : 91.66666666666666 %\n",
      "Loss after iteration 1000 : 0.1618732586379986\n",
      "Accuracy after iteration 1000 : 94.16666666666667 %\n",
      "Loss after iteration 1100 : 0.14662427850938495\n",
      "Accuracy after iteration 1100 : 95.0 %\n",
      "Loss after iteration 1200 : 0.1359412882049441\n",
      "Accuracy after iteration 1200 : 95.0 %\n",
      "Loss after iteration 1300 : 0.12727806955432838\n",
      "Accuracy after iteration 1300 : 95.0 %\n",
      "Loss after iteration 1400 : 0.1198403104518893\n",
      "Accuracy after iteration 1400 : 94.16666666666667 %\n",
      "Loss after iteration 1500 : 0.11328454572031303\n",
      "Accuracy after iteration 1500 : 94.16666666666667 %\n",
      "Loss after iteration 1600 : 0.10734248360802165\n",
      "Accuracy after iteration 1600 : 94.16666666666667 %\n",
      "Loss after iteration 1700 : 0.10157673064581745\n",
      "Accuracy after iteration 1700 : 93.33333333333333 %\n",
      "Loss after iteration 1800 : 0.09562187434043139\n",
      "Accuracy after iteration 1800 : 93.33333333333333 %\n",
      "Loss after iteration 1900 : 0.09071978097916647\n",
      "Accuracy after iteration 1900 : 93.33333333333333 %\n",
      "Loss after iteration 2000 : 0.08666265990741483\n",
      "Accuracy after iteration 2000 : 93.33333333333333 %\n",
      "Loss after iteration 2100 : 0.08294494989647051\n",
      "Accuracy after iteration 2100 : 94.16666666666667 %\n",
      "Loss after iteration 2200 : 0.07940497650108003\n",
      "Accuracy after iteration 2200 : 94.16666666666667 %\n",
      "Loss after iteration 2300 : 0.07598721618807631\n",
      "Accuracy after iteration 2300 : 94.16666666666667 %\n",
      "Loss after iteration 2400 : 0.07267604926411414\n",
      "Accuracy after iteration 2400 : 94.16666666666667 %\n",
      "Loss after iteration 2500 : 0.06946770216932566\n",
      "Accuracy after iteration 2500 : 95.0 %\n",
      "Loss after iteration 2600 : 0.06635627924754418\n",
      "Accuracy after iteration 2600 : 95.0 %\n",
      "Loss after iteration 2700 : 0.06333010180660402\n",
      "Accuracy after iteration 2700 : 95.0 %\n",
      "Loss after iteration 2800 : 0.060377336366054364\n",
      "Accuracy after iteration 2800 : 95.0 %\n",
      "Loss after iteration 2900 : 0.05750008040358127\n",
      "Accuracy after iteration 2900 : 96.66666666666667 %\n",
      "Loss after iteration 3000 : 0.05472904703169604\n",
      "Accuracy after iteration 3000 : 97.5 %\n",
      "Loss after iteration 3100 : 0.05212014286888821\n",
      "Accuracy after iteration 3100 : 97.5 %\n",
      "Loss after iteration 3200 : 0.04972723505125835\n",
      "Accuracy after iteration 3200 : 98.33333333333333 %\n",
      "Loss after iteration 3300 : 0.04757745158801523\n",
      "Accuracy after iteration 3300 : 98.33333333333333 %\n",
      "Loss after iteration 3400 : 0.0456693152663731\n",
      "Accuracy after iteration 3400 : 98.33333333333333 %\n",
      "Loss after iteration 3500 : 0.04398414103078528\n",
      "Accuracy after iteration 3500 : 98.33333333333333 %\n",
      "Loss after iteration 3600 : 0.04249666429209219\n",
      "Accuracy after iteration 3600 : 98.33333333333333 %\n",
      "Loss after iteration 3700 : 0.041180954605488033\n",
      "Accuracy after iteration 3700 : 98.33333333333333 %\n",
      "Loss after iteration 3800 : 0.04001294284482708\n",
      "Accuracy after iteration 3800 : 98.33333333333333 %\n",
      "Loss after iteration 3900 : 0.03897127742413547\n",
      "Accuracy after iteration 3900 : 98.33333333333333 %\n",
      "Loss after iteration 4000 : 0.03803746282987904\n",
      "Accuracy after iteration 4000 : 98.33333333333333 %\n",
      "Loss after iteration 4100 : 0.03719569629139537\n",
      "Accuracy after iteration 4100 : 98.33333333333333 %\n",
      "Loss after iteration 4200 : 0.036432581818514796\n",
      "Accuracy after iteration 4200 : 98.33333333333333 %\n",
      "Loss after iteration 4300 : 0.035736808535623575\n",
      "Accuracy after iteration 4300 : 98.33333333333333 %\n",
      "Loss after iteration 4400 : 0.03509883804320262\n",
      "Accuracy after iteration 4400 : 98.33333333333333 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e2892476a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = initialize_parameters(nn_input_dim=4, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X_train,Y_train,learning_rate=0.05,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  93.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "test = predict(model,X_test)\n",
    "test = pd.get_dummies(test)\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
