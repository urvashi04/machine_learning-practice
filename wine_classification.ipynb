{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "8bec90e6-23f2-4aae-8b4a-e9ef999391b7",
    "_uuid": "ae07d14dcb75d8188c49d57cdc54fc36f3397d02"
   },
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('wine.csv')\n",
    "# print(df)\n",
    "a = pd.get_dummies(df['Wine'])\n",
    "df = pd.concat([df,a],axis=1)\n",
    "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
    "y = df[[1,2,3]].values\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Y_test,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0     1    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1     1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2     1    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "3     1    14.37        1.95  2.50  16.8  113     3.85        3.49   \n",
       "4     1    13.24        2.59  2.87  21.0  118     2.80        2.69   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  1  2  3  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  1  0  0  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  1  0  0  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  1  0  0  \n",
       "3                  0.24     2.18       7.80  0.86  3.45     1480  1  0  0  \n",
       "4                  0.39     1.82       4.32  1.04  2.93      735  1  0  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "173     3    13.71        5.65  2.45  20.5   95     1.68        0.61   \n",
       "174     3    13.40        3.91  2.48  23.0  102     1.80        0.75   \n",
       "175     3    13.27        4.28  2.26  20.0  120     1.59        0.69   \n",
       "176     3    13.17        2.59  2.37  20.0  120     1.65        0.68   \n",
       "177     3    14.13        4.10  2.74  24.5   96     2.05        0.76   \n",
       "\n",
       "     Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  1  2  3  \n",
       "173                  0.52     1.06        7.7  0.64  1.74      740  0  0  1  \n",
       "174                  0.43     1.41        7.3  0.70  1.56      750  0  0  1  \n",
       "175                  0.43     1.35       10.2  0.59  1.56      835  0  0  1  \n",
       "176                  0.53     1.46        9.3  0.60  1.62      840  0  0  1  \n",
       "177                  0.56     1.35        9.2  0.61  1.60      560  0  0  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(model,a0):\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Objective/Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and activation derivative for backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly initialize all Neural Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model parameters and train model on wine dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "90023c7a-8059-4117-98b3-82ceb3e5877a",
    "_uuid": "de660f4b64992b03558fc14ee176479b4b7afc29",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.267199066674147\n",
      "Accuracy after iteration 0 : 38.028169014084504 %\n",
      "Loss after iteration 100 : 0.7315540944843346\n",
      "Accuracy after iteration 100 : 67.6056338028169 %\n",
      "Loss after iteration 200 : 0.6092206537140172\n",
      "Accuracy after iteration 200 : 73.94366197183099 %\n",
      "Loss after iteration 300 : 0.5319504490541479\n",
      "Accuracy after iteration 300 : 77.46478873239437 %\n",
      "Loss after iteration 400 : 0.4726153228462911\n",
      "Accuracy after iteration 400 : 81.69014084507043 %\n",
      "Loss after iteration 500 : 0.4308743130303828\n",
      "Accuracy after iteration 500 : 83.09859154929578 %\n",
      "Loss after iteration 600 : 0.3940473344741446\n",
      "Accuracy after iteration 600 : 84.50704225352112 %\n",
      "Loss after iteration 700 : 0.3729106460179943\n",
      "Accuracy after iteration 700 : 85.91549295774648 %\n",
      "Loss after iteration 800 : 0.35323533610913077\n",
      "Accuracy after iteration 800 : 87.32394366197182 %\n",
      "Loss after iteration 900 : 0.33278061513235063\n",
      "Accuracy after iteration 900 : 88.02816901408451 %\n",
      "Loss after iteration 1000 : 0.3177232439712677\n",
      "Accuracy after iteration 1000 : 88.02816901408451 %\n",
      "Loss after iteration 1100 : 0.301547280774653\n",
      "Accuracy after iteration 1100 : 89.43661971830986 %\n",
      "Loss after iteration 1200 : 0.28508396507570427\n",
      "Accuracy after iteration 1200 : 89.43661971830986 %\n",
      "Loss after iteration 1300 : 0.26842979298538366\n",
      "Accuracy after iteration 1300 : 89.43661971830986 %\n",
      "Loss after iteration 1400 : 0.2568866306445019\n",
      "Accuracy after iteration 1400 : 90.14084507042254 %\n",
      "Loss after iteration 1500 : 0.24971870988579867\n",
      "Accuracy after iteration 1500 : 90.14084507042254 %\n",
      "Loss after iteration 1600 : 0.24452722873072896\n",
      "Accuracy after iteration 1600 : 90.14084507042254 %\n",
      "Loss after iteration 1700 : 0.2397472287953578\n",
      "Accuracy after iteration 1700 : 90.84507042253522 %\n",
      "Loss after iteration 1800 : 0.2334762826794537\n",
      "Accuracy after iteration 1800 : 90.84507042253522 %\n",
      "Loss after iteration 1900 : 0.22355173089194488\n",
      "Accuracy after iteration 1900 : 91.54929577464789 %\n",
      "Loss after iteration 2000 : 0.2156818328318233\n",
      "Accuracy after iteration 2000 : 91.54929577464789 %\n",
      "Loss after iteration 2100 : 0.20971658979122354\n",
      "Accuracy after iteration 2100 : 92.25352112676056 %\n",
      "Loss after iteration 2200 : 0.20309029956120403\n",
      "Accuracy after iteration 2200 : 92.25352112676056 %\n",
      "Loss after iteration 2300 : 0.19538720724367148\n",
      "Accuracy after iteration 2300 : 92.95774647887323 %\n",
      "Loss after iteration 2400 : 0.18983787215166564\n",
      "Accuracy after iteration 2400 : 92.95774647887323 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x227f39959b0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X_train,Y_train,learning_rate=0.04,epochs=2500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calculate testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "cf85a92b-2bc6-4b57-8cd6-53b523add48a",
    "_uuid": "6aca14a72b394b8bc0d83fae7134fd4f8bcbfce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  94.44444444444444%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = predict(model,X_test)\n",
    "test = pd.get_dummies(test)\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
